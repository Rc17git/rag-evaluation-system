Model pruning removes less important weights or neurons from a neural network to reduce size and computation. The goal is to create a smaller, faster model while maintaining acceptable accuracy. Pruning can be structured (removing entire channels or layers) or unstructured (removing individual weights), with structured pruning typically offering better hardware efficiency.
Pruning is usually applied after training, followed by fine-tuning to recover performance. Criteria for pruning may include weight magnitude, gradient sensitivity, or learned importance scores. Iterative pruning strategies gradually remove parameters and retrain the model to maintain stability.
When combined with quantization and other optimization techniques, pruning can significantly improve deployment efficiency. It is particularly useful for edge devices or CPU-only environments where compute resources are limited. Proper evaluation ensures that the reduced model still meets performance and accuracy requirements.