Gradient checkpointing is a memory optimization technique used during training to reduce peak memory usage. Instead of storing all intermediate activations for backpropagation, the method saves only selected checkpoints and recomputes other activations during the backward pass. This trades additional computation for lower memory consumption, enabling training of larger models on limited hardware.
The technique is especially useful for deep architectures like transformers, where activation storage can dominate memory usage. By strategically placing checkpoints, developers can control the balance between memory savings and recomputation overhead. Frameworks like PyTorch provide built-in support for gradient checkpointing, making it easier to integrate into training pipelines.
While gradient checkpointing increases training time slightly due to recomputation, it allows experiments with larger batch sizes or deeper networks that would otherwise be infeasible. Proper profiling helps determine whether the memory savings justify the added compute cost. In large-scale model training, it is a widely used strategy for managing resource constraints.