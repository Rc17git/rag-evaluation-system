Quantization reduces model size and computation cost by representing weights and activations with lower-precision data types, such as int8 instead of float32. This technique can significantly improve inference speed and memory efficiency, particularly on CPUs where integer operations are often faster and more cache-friendly. Quantization can be applied post-training or during training (quantization-aware training).
Post-training quantization converts a trained model to lower precision with minimal additional effort. While simple to implement, it may introduce accuracy degradation depending on the model and task. Quantization-aware training simulates low-precision arithmetic during training, allowing the model to adapt and retain higher accuracy after conversion. Both approaches aim to balance efficiency with performance.
In deployment pipelines, quantization is often combined with operator fusion and optimized runtimes. Careful calibration and evaluation are necessary to ensure acceptable accuracy levels. When applied correctly, quantization can reduce model size by multiple times and improve latency, making it a core technique for efficient inference.